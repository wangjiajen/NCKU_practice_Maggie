{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seeds(seed=42)\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer): # Transformer的Encoder端，Transformer block塊\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att=layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn=keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
    "        self.layernorm1=layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2=layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1=layers.Dropout(rate)\n",
    "        self.dropout2=layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        attn_output=self.att(inputs, inputs)\n",
    "        attn_output=self.dropout1(attn_output, training=training)\n",
    "        out1=self.layernorm1(inputs + attn_output)\n",
    "        ffn_output=self.ffn(out1)\n",
    "        ffn_output=self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClassIndex</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ClassIndex                                            summary\n",
       "0           3  Wall St. Bears Claw Back Into the Black (Reute...\n",
       "1           3  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2           3  Oil and Economy Cloud Stocks' Outlook (Reuters...\n",
       "3           3  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4           3  Oil prices soar to all-time record, posing new..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/home/u108029050/m/train.csv')\n",
    "testdata = pd.read_csv('/home/u108029050/m/test.csv')\n",
    "\n",
    "#Set Column Names \n",
    "data.columns = ['ClassIndex', 'Title', 'Description']\n",
    "testdata.columns = ['ClassIndex', 'Title', 'Description']\n",
    "\n",
    "data['summary'] = data['Title'] + ' ' + data['Description']\n",
    "testdata['summary'] = testdata['Title'] + ' ' + testdata['Description']\n",
    "\n",
    "data = data.drop(columns=['Title', 'Description'])\n",
    "testdata = testdata.drop(columns=['Title', 'Description'])\n",
    "\n",
    "\n",
    "#Combine Title and Description\n",
    "X_train = data['summary'] # Combine title and description (better accuracy than using them as separate features)\n",
    "y_train = data['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n",
    "x_test = testdata['summary'] # Combine title and description (better accuracy than using them as separate features)\n",
    "y_test = testdata['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n",
    "\n",
    "#Max Length of sentences in Train Dataset\n",
    "maxlen = X_train.map(lambda x: len(x.split())).max()\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120000, 2), (7600, 2))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, testdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train,4)\n",
    "y_test = to_categorical(y_test,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70337 unique tokens\n"
     ]
    }
   ],
   "source": [
    "max_words = 10000 # 僅考慮資料集中的前10000個單詞\n",
    "maxlen = 100 # 100個文字後切斷評論\n",
    "# Create and Fit tokenizer\n",
    "\n",
    "tok = Tokenizer(num_words=max_words) # 實例化一個只考慮最常用10000詞的分詞器\n",
    "tok.fit_on_texts(X_train.values) # 建構單詞索引\n",
    "# vocab_size = len(tok.word_index) + 1\n",
    "\n",
    "# 將文字轉成整數list的序列資料\n",
    "X_train = tok.texts_to_sequences(X_train)\n",
    "x_test = tok.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad data\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "word_index = tok.word_index #單詞和數字的字典\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "# print(len(X_train), \"Training sequences\")\n",
    "# print(len(x_test), \"Validation sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 107803 word vectors\n",
      "[-0.81419706 -0.8000132   2.0608513  -1.007846   -0.7216044  -0.8566915\n",
      "  0.7966929   3.7051861  -0.46912885 -0.68854475 -1.3169966  -0.50396204\n",
      " -1.2225083   0.9640229  -0.946806    0.18398409 -0.948219    1.6240733\n",
      "  0.06061047 -0.76218134  0.10111515 -0.41588673  0.32345408 -0.19489264\n",
      "  0.4045541  -0.18221259  0.48027515 -0.84279644  0.3509806   2.6030517\n",
      "  1.8096178   0.473035   -0.8081082   0.58772033 -0.8069067   0.23847212\n",
      "  0.7584653   0.24641363 -0.85608065 -1.5835495   0.19089963 -0.3917458\n",
      " -2.0202892   0.2806195   0.21958712 -1.1981602  -0.45732456 -2.6616156\n",
      "  0.42792758  1.4582756  -0.7322122   0.11539538  1.5570602   0.11495335\n",
      "  0.74106705  0.83484536  0.42752406  1.6985508  -1.4286654   1.9803507\n",
      "  0.39201847 -1.3146726  -1.061342    1.49797     0.8115893  -0.33302578\n",
      "  3.2861745  -1.6020184  -1.0791559   0.08012582 -1.0471542   1.683749\n",
      " -0.4148051   0.39994523 -0.33512256  3.4928787  -0.60213983 -0.72856414\n",
      " -0.5789077   1.5414579  -0.72842103 -1.3555213   2.5683992   0.09521607\n",
      " -0.31384066 -0.6775084  -0.89607096  0.19438256 -2.9951718  -0.06449945\n",
      " -1.7933198  -1.2311066  -0.7033014  -3.0786967   0.94369733 -0.7767153\n",
      " -1.7386246  -0.55430835  2.4735615  -0.396945  ]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "embedding_index = {}\n",
    "f = open('wiki.txt')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embedding_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors' % len(embedding_index))\n",
    "print(embedding_index[\"google\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if i < max_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, max_words, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb=layers.Embedding(input_dim=max_words, output_dim=embed_dim,weights=[embedding_matrix],trainable=False)\n",
    "        self.pos_emb=layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "    def call(self, x):\n",
    "        maxlen=tf.shape(x)[-1]\n",
    "        positions=tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions=self.pos_emb(positions)\n",
    "        x=self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100  # 嵌入向量總長度\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 100  # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 100, 100)         1010000   \n",
      " g_4 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block_4 (Transf  (None, 100, 100)         101300    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " global_average_pooling1d_4   (None, 100)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 20)                2020      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 84        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,113,404\n",
      "Trainable params: 113,404\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, max_words, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 100)\n",
      "(120000, 4)\n",
      "(7600, 4)\n",
      "(7600, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(X_train)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "188/188 [==============================] - 99s 525ms/step - loss: 0.5154 - accuracy: 0.8175 - val_loss: 0.3664 - val_accuracy: 0.8707\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - 100s 530ms/step - loss: 0.3891 - accuracy: 0.8659 - val_loss: 0.3397 - val_accuracy: 0.8795\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - 100s 531ms/step - loss: 0.3634 - accuracy: 0.8744 - val_loss: 0.3242 - val_accuracy: 0.8834\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - 101s 536ms/step - loss: 0.3486 - accuracy: 0.8792 - val_loss: 0.3207 - val_accuracy: 0.8856\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - 99s 526ms/step - loss: 0.3358 - accuracy: 0.8833 - val_loss: 0.3221 - val_accuracy: 0.8847\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - 99s 526ms/step - loss: 0.3256 - accuracy: 0.8857 - val_loss: 0.3100 - val_accuracy: 0.8891\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - 99s 529ms/step - loss: 0.3159 - accuracy: 0.8896 - val_loss: 0.3048 - val_accuracy: 0.8925\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - 104s 555ms/step - loss: 0.3110 - accuracy: 0.8916 - val_loss: 0.3003 - val_accuracy: 0.8935\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - 151s 806ms/step - loss: 0.3024 - accuracy: 0.8931 - val_loss: 0.3042 - val_accuracy: 0.8912\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - 151s 806ms/step - loss: 0.2981 - accuracy: 0.8951 - val_loss: 0.2991 - val_accuracy: 0.8951\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history=model.fit(X_train, y_train, batch_size=512, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 4s 15ms/step - loss: 0.3263 - accuracy: 0.8839\n",
      "\n",
      "accuracy: 88.39%\n"
     ]
    }
   ],
   "source": [
    "history.history\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 3s 10ms/step\n",
      "Group to Propose New High-Speed Wireless Format  L ...\n",
      "Actual category:  Science-Technology News\n",
      "predicted category:  Science-Technology News\n",
      "Socialites unite dolphin groups Dolphin groups, or ...\n",
      "Actual category:  Science-Technology News\n",
      "predicted category:  Science-Technology News\n",
      "Rocking the Cradle of Life When did life begin? On ...\n",
      "Actual category:  Science-Technology News\n",
      "predicted category:  Science-Technology News\n",
      "IBM Chips May Someday Heal Themselves New technolo ...\n",
      "Actual category:  Science-Technology News\n",
      "predicted category:  Science-Technology News\n",
      "Giddy Phelps Touches Gold for First Time Michael P ...\n",
      "Actual category:  Sports News\n",
      "predicted category:  Sports News\n",
      "They've caught his eye In  quot;helping themselves ...\n",
      "Actual category:  Sports News\n",
      "predicted category:  Sports News\n",
      "Explosions Echo Throughout Najaf NAJAF, Iraq - Exp ...\n",
      "Actual category:  World News\n",
      "predicted category:  World News\n",
      "Iran Warns Its Missiles Can Hit Anywhere in Israel ...\n",
      "Actual category:  World News\n",
      "predicted category:  World News\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(x_test)\n",
    "labels = ['World News', 'Sports News', 'Business News', 'Science-Technology News']\n",
    "for i in range(10,40,4):\n",
    "    print(testdata['summary'].iloc[i][:50], \"...\")\n",
    "    print(\"Actual category: \", labels[np.argmax(y_test[i])])\n",
    "    print(\"predicted category: \",labels[np.argmax(prediction[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1636   81  127   56]\n",
      " [  22 1838   28   12]\n",
      " [  61   31 1626  182]\n",
      " [  65   26  191 1618]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      1900\n",
      "           1       0.93      0.97      0.95      1900\n",
      "           2       0.82      0.86      0.84      1900\n",
      "           3       0.87      0.85      0.86      1900\n",
      "\n",
      "    accuracy                           0.88      7600\n",
      "   macro avg       0.88      0.88      0.88      7600\n",
      "weighted avg       0.88      0.88      0.88      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "y_test_arg=np.argmax(y_test,axis=1)\n",
    "Y_pred = np.argmax(prediction,axis=1)\n",
    "print(confusion_matrix(y_test_arg, Y_pred)) #y軸事實 x軸預測\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_arg, Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfe555e69db9cf0413f7ceabec452a2f6bfcadc3ede131195c47fb5f38c9a69c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
